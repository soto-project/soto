//===----------------------------------------------------------------------===//
//
// This source file is part of the Soto for AWS open source project
//
// Copyright (c) 2017-2024 the Soto project authors
// Licensed under Apache License v2.0
//
// See LICENSE.txt for license information
// See CONTRIBUTORS.txt for the list of Soto project authors
//
// SPDX-License-Identifier: Apache-2.0
//
//===----------------------------------------------------------------------===//

// THIS FILE IS AUTOMATICALLY GENERATED by https://github.com/soto-project/soto-codegenerator.
// DO NOT EDIT.

#if canImport(FoundationEssentials)
import FoundationEssentials
#else
import Foundation
#endif
@_exported import SotoCore

/// Service object for interacting with AWS TranscribeStreaming service.
///
/// Amazon Transcribe streaming offers four main types of real-time transcription: Standard, Medical, Call Analytics, and Health Scribe.    Standard transcriptions are the most common option. Refer to  for details.    Medical transcriptions are tailored to medical professionals  and incorporate medical terms. A common use case for this service is transcribing doctor-patient  dialogue in real time, so doctors can focus on their patient instead of taking notes. Refer to for details.    Call Analytics transcriptions are designed for use with call center audio on two different channels; if you're looking for insight into customer service calls, use this  option. Refer to  for details.    HealthScribe transcriptions are designed to automatically create clinical notes from patient-clinician conversations using generative AI. Refer to [here] for details.
public struct TranscribeStreaming: AWSService {
    // MARK: Member variables

    /// Client used for communication with AWS
    public let client: AWSClient
    /// Service configuration
    public let config: AWSServiceConfig

    // MARK: Initialization

    /// Initialize the TranscribeStreaming client
    /// - parameters:
    ///     - client: AWSClient used to process requests
    ///     - region: Region of server you want to communicate with. This will override the partition parameter.
    ///     - partition: AWS partition where service resides, standard (.aws), china (.awscn), government (.awsusgov).
    ///     - endpoint: Custom endpoint URL to use instead of standard AWS servers
    ///     - middleware: Middleware chain used to edit requests before they are sent and responses before they are decoded 
    ///     - timeout: Timeout value for HTTP requests
    ///     - byteBufferAllocator: Allocator for ByteBuffers
    ///     - options: Service options
    public init(
        client: AWSClient,
        region: SotoCore.Region? = nil,
        partition: AWSPartition = .aws,
        endpoint: String? = nil,
        middleware: AWSMiddlewareProtocol? = nil,
        timeout: TimeAmount? = nil,
        byteBufferAllocator: ByteBufferAllocator = ByteBufferAllocator(),
        options: AWSServiceConfig.Options = []
    ) {
        self.client = client
        self.config = AWSServiceConfig(
            region: region,
            partition: region?.partition ?? partition,
            serviceName: "TranscribeStreaming",
            serviceIdentifier: "transcribestreaming",
            signingName: "transcribe",
            serviceProtocol: .restjson,
            apiVersion: "2017-10-26",
            endpoint: endpoint,
            variantEndpoints: Self.variantEndpoints,
            errorType: TranscribeStreamingErrorType.self,
            middleware: middleware,
            timeout: timeout,
            byteBufferAllocator: byteBufferAllocator,
            options: options
        )
    }




    /// FIPS and dualstack endpoints
    static var variantEndpoints: [EndpointVariantType: AWSServiceConfig.EndpointVariant] {[
        [.dualstack]: .init(endpoints: [
            "af-south-1": "transcribestreaming.af-south-1.api.aws",
            "ap-northeast-1": "transcribestreaming.ap-northeast-1.api.aws",
            "ap-northeast-2": "transcribestreaming.ap-northeast-2.api.aws",
            "ap-south-1": "transcribestreaming.ap-south-1.api.aws",
            "ap-southeast-1": "transcribestreaming.ap-southeast-1.api.aws",
            "ap-southeast-2": "transcribestreaming.ap-southeast-2.api.aws",
            "ca-central-1": "transcribestreaming.ca-central-1.api.aws",
            "cn-north-1": "transcribestreaming.cn-north-1.api.amazonwebservices.com.cn",
            "cn-northwest-1": "transcribestreaming.cn-northwest-1.api.amazonwebservices.com.cn",
            "eu-central-1": "transcribestreaming.eu-central-1.api.aws",
            "eu-west-1": "transcribestreaming.eu-west-1.api.aws",
            "eu-west-2": "transcribestreaming.eu-west-2.api.aws",
            "sa-east-1": "transcribestreaming.sa-east-1.api.aws",
            "us-east-1": "transcribestreaming.us-east-1.api.aws",
            "us-east-2": "transcribestreaming.us-east-2.api.aws",
            "us-gov-east-1": "transcribestreaming.us-gov-east-1.api.aws",
            "us-gov-west-1": "transcribestreaming.us-gov-west-1.api.aws",
            "us-west-2": "transcribestreaming.us-west-2.api.aws"
        ]),
        [.dualstack, .fips]: .init(endpoints: [
            "ca-central-1": "transcribestreaming-fips.ca-central-1.api.aws",
            "us-east-1": "transcribestreaming-fips.us-east-1.api.aws",
            "us-east-2": "transcribestreaming-fips.us-east-2.api.aws",
            "us-gov-east-1": "transcribestreaming-fips.us-gov-east-1.api.aws",
            "us-gov-west-1": "transcribestreaming-fips.us-gov-west-1.api.aws",
            "us-west-2": "transcribestreaming-fips.us-west-2.api.aws"
        ]),
        [.fips]: .init(endpoints: [
            "ca-central-1": "transcribestreaming-fips.ca-central-1.amazonaws.com",
            "us-east-1": "transcribestreaming-fips.us-east-1.amazonaws.com",
            "us-east-2": "transcribestreaming-fips.us-east-2.amazonaws.com",
            "us-gov-east-1": "transcribestreaming-fips.us-gov-east-1.amazonaws.com",
            "us-gov-west-1": "transcribestreaming-fips.us-gov-west-1.amazonaws.com",
            "us-west-2": "transcribestreaming-fips.us-west-2.amazonaws.com"
        ])
    ]}

    // MARK: API Calls

    /// Provides details about the specified Amazon Web Services HealthScribe streaming session. To view the status of the streaming session, check the StreamStatus field in the response. To get the details of post-stream analytics, including its status, check the PostStreamAnalyticsResult field in the response.
    @Sendable
    @inlinable
    public func getMedicalScribeStream(_ input: GetMedicalScribeStreamRequest, logger: Logger = AWSClient.loggingDisabled) async throws -> GetMedicalScribeStreamResponse {
        try await self.client.execute(
            operation: "GetMedicalScribeStream", 
            path: "/medical-scribe-stream/{SessionId}", 
            httpMethod: .GET, 
            serviceConfig: self.config, 
            input: input, 
            logger: logger
        )
    }
    /// Provides details about the specified Amazon Web Services HealthScribe streaming session. To view the status of the streaming session, check the StreamStatus field in the response. To get the details of post-stream analytics, including its status, check the PostStreamAnalyticsResult field in the response.
    ///
    /// Parameters:
    ///   - sessionId: The identifier of the HealthScribe streaming session you want information about.
    ///   - logger: Logger use during operation
    @inlinable
    public func getMedicalScribeStream(
        sessionId: String,
        logger: Logger = AWSClient.loggingDisabled        
    ) async throws -> GetMedicalScribeStreamResponse {
        let input = GetMedicalScribeStreamRequest(
            sessionId: sessionId
        )
        return try await self.getMedicalScribeStream(input, logger: logger)
    }

    /// Starts a bidirectional HTTP/2 or WebSocket stream where audio is streamed to  Amazon Transcribe and the transcription results are streamed to your application. Use this operation for Call Analytics transcriptions. The following parameters are required:    language-code     media-encoding     sample-rate    For more information on streaming with Amazon Transcribe, see Transcribing streaming audio.
    @Sendable
    @inlinable
    public func startCallAnalyticsStreamTranscription(_ input: StartCallAnalyticsStreamTranscriptionRequest, logger: Logger = AWSClient.loggingDisabled) async throws -> StartCallAnalyticsStreamTranscriptionResponse {
        try await self.client.execute(
            operation: "StartCallAnalyticsStreamTranscription", 
            path: "/call-analytics-stream-transcription", 
            httpMethod: .POST, 
            serviceConfig: self.config, 
            input: input, 
            logger: logger
        )
    }
    /// Starts a bidirectional HTTP/2 or WebSocket stream where audio is streamed to  Amazon Transcribe and the transcription results are streamed to your application. Use this operation for Call Analytics transcriptions. The following parameters are required:    language-code     media-encoding     sample-rate    For more information on streaming with Amazon Transcribe, see Transcribing streaming audio.
    ///
    /// Parameters:
    ///   - audioStream: An encoded stream of audio blobs. Audio streams are encoded as either HTTP/2 or WebSocket  data frames. For more information, see Transcribing streaming audio.
    ///   - contentIdentificationType: Labels all personally identifiable information (PII) identified in your transcript. Content identification is performed at the segment level; PII specified in  PiiEntityTypes is flagged upon complete transcription of an audio segment. If you don't include PiiEntityTypes in your request, all PII is identified. You canâ€™t set ContentIdentificationType and ContentRedactionType in the same request. If you set both, your request returns a BadRequestException. For more information, see Redacting or identifying personally identifiable information.
    ///   - contentRedactionType: Redacts all personally identifiable information (PII) identified in your transcript. Content redaction is performed at the segment level; PII specified in  PiiEntityTypes is redacted upon complete transcription of an audio segment. If you don't include PiiEntityTypes in your request, all PII is redacted. You canâ€™t set ContentRedactionType and ContentIdentificationType in the same request. If you set both, your request returns a BadRequestException. For more information, see Redacting or identifying personally identifiable information.
    ///   - enablePartialResultsStabilization: Enables partial result stabilization for your transcription. Partial result stabilization can reduce latency in your output, but may impact accuracy. For more information, see  Partial-result  stabilization.
    ///   - languageCode: Specify the language code that represents the language spoken in your audio. For a list of languages supported with real-time Call Analytics, refer to the  Supported  languages table.
    ///   - languageModelName: Specify the name of the custom language model that you want to use when processing your transcription. Note that language model names are case sensitive. The language of the specified language model must match the language code you specify in your transcription request. If the languages don't match, the custom language model isn't applied.  There are no errors or warnings associated with a language mismatch. For more information, see Custom language models.
    ///   - mediaEncoding: Specify the encoding of your input audio. Supported formats are:   FLAC   OPUS-encoded audio in an Ogg container   PCM (only signed 16-bit little-endian audio formats, which does not include WAV)   For more information, see Media formats.
    ///   - mediaSampleRateHertz: The sample rate of the input audio (in hertz). Low-quality audio, such as telephone audio, is typically around 8,000 Hz. High-quality audio typically ranges from 16,000 Hz to 48,000 Hz. Note that the sample rate you specify must match that of your audio.
    ///   - partialResultsStability: Specify the level of stability to use when you enable partial results stabilization  (EnablePartialResultsStabilization). Low stability provides the highest accuracy. High stability transcribes faster, but with slightly lower accuracy. For more information, see Partial-result  stabilization.
    ///   - piiEntityTypes: Specify which types of personally identifiable information (PII) you want to redact in your  transcript. You can include as many types as you'd like, or you can select  ALL. Values must be comma-separated and can include: ADDRESS,  BANK_ACCOUNT_NUMBER, BANK_ROUTING, CREDIT_DEBIT_CVV, CREDIT_DEBIT_EXPIRY, CREDIT_DEBIT_NUMBER, EMAIL,  NAME, PHONE, PIN,  SSN, or ALL. Note that if you include PiiEntityTypes in your request, you must also include  ContentIdentificationType or ContentRedactionType. If you include ContentRedactionType or  ContentIdentificationType in your request, but do not include  PiiEntityTypes, all PII is redacted or identified.
    ///   - sessionId: Specify a name for your Call Analytics transcription session. If you don't include this parameter in your request, Amazon Transcribe generates an ID and returns it in the response.
    ///   - vocabularyFilterMethod: Specify how you want your vocabulary filter applied to your transcript. To replace words with ***, choose mask. To delete words, choose remove. To flag words without changing them, choose tag.
    ///   - vocabularyFilterName: Specify the name of the custom vocabulary filter that you want to use when processing your transcription. Note that vocabulary filter names are case sensitive. If the language of the specified custom vocabulary filter doesn't match the language identified in your media, the vocabulary filter is not applied to your transcription. For more information, see Using vocabulary filtering with unwanted  words.
    ///   - vocabularyName: Specify the name of the custom vocabulary that you want to use when processing your transcription. Note that vocabulary names are case sensitive. If the language of the specified custom vocabulary doesn't match the language identified in your media, the custom vocabulary is not applied to your transcription. For more information, see Custom vocabularies.
    ///   - logger: Logger use during operation
    @inlinable
    public func startCallAnalyticsStreamTranscription(
        audioStream: AWSEventStream<AudioStream>,
        contentIdentificationType: ContentIdentificationType? = nil,
        contentRedactionType: ContentRedactionType? = nil,
        enablePartialResultsStabilization: Bool? = nil,
        languageCode: CallAnalyticsLanguageCode,
        languageModelName: String? = nil,
        mediaEncoding: MediaEncoding,
        mediaSampleRateHertz: Int,
        partialResultsStability: PartialResultsStability? = nil,
        piiEntityTypes: String? = nil,
        sessionId: String? = nil,
        vocabularyFilterMethod: VocabularyFilterMethod? = nil,
        vocabularyFilterName: String? = nil,
        vocabularyName: String? = nil,
        logger: Logger = AWSClient.loggingDisabled        
    ) async throws -> StartCallAnalyticsStreamTranscriptionResponse {
        let input = StartCallAnalyticsStreamTranscriptionRequest(
            audioStream: audioStream, 
            contentIdentificationType: contentIdentificationType, 
            contentRedactionType: contentRedactionType, 
            enablePartialResultsStabilization: enablePartialResultsStabilization, 
            languageCode: languageCode, 
            languageModelName: languageModelName, 
            mediaEncoding: mediaEncoding, 
            mediaSampleRateHertz: mediaSampleRateHertz, 
            partialResultsStability: partialResultsStability, 
            piiEntityTypes: piiEntityTypes, 
            sessionId: sessionId, 
            vocabularyFilterMethod: vocabularyFilterMethod, 
            vocabularyFilterName: vocabularyFilterName, 
            vocabularyName: vocabularyName
        )
        return try await self.startCallAnalyticsStreamTranscription(input, logger: logger)
    }

    /// Starts a bidirectional HTTP/2 stream, where audio is streamed to Amazon Web Services HealthScribe and the transcription results are streamed to your application. When you start a stream, you first specify the stream configuration in a MedicalScribeConfigurationEvent.  This event includes channel definitions, encryption settings, and post-stream analytics settings, such as the output configuration for aggregated transcript and clinical note generation. These are additional streaming session configurations beyond those provided in your initial start request headers. Whether you are starting a new session or resuming an existing session,  your first event must be a MedicalScribeConfigurationEvent.   After you send a MedicalScribeConfigurationEvent, you start AudioEvents and Amazon Web Services HealthScribe  responds with real-time transcription results. When you are finished, to start processing the results with the post-stream analytics, send a MedicalScribeSessionControlEvent with a Type of  END_OF_SESSION and Amazon Web Services HealthScribe starts the analytics.  You can pause or resume streaming. To pause streaming, complete the input stream without sending the MedicalScribeSessionControlEvent. To resume streaming, call the StartMedicalScribeStream and specify the same SessionId you used to start the stream.  The following parameters are required:    language-code     media-encoding     media-sample-rate-hertz     For more information on streaming with Amazon Web Services HealthScribe, see Amazon Web Services HealthScribe.
    @Sendable
    @inlinable
    public func startMedicalScribeStream(_ input: StartMedicalScribeStreamRequest, logger: Logger = AWSClient.loggingDisabled) async throws -> StartMedicalScribeStreamResponse {
        try await self.client.execute(
            operation: "StartMedicalScribeStream", 
            path: "/medical-scribe-stream", 
            httpMethod: .POST, 
            serviceConfig: self.config, 
            input: input, 
            logger: logger
        )
    }
    /// Starts a bidirectional HTTP/2 stream, where audio is streamed to Amazon Web Services HealthScribe and the transcription results are streamed to your application. When you start a stream, you first specify the stream configuration in a MedicalScribeConfigurationEvent.  This event includes channel definitions, encryption settings, and post-stream analytics settings, such as the output configuration for aggregated transcript and clinical note generation. These are additional streaming session configurations beyond those provided in your initial start request headers. Whether you are starting a new session or resuming an existing session,  your first event must be a MedicalScribeConfigurationEvent.   After you send a MedicalScribeConfigurationEvent, you start AudioEvents and Amazon Web Services HealthScribe  responds with real-time transcription results. When you are finished, to start processing the results with the post-stream analytics, send a MedicalScribeSessionControlEvent with a Type of  END_OF_SESSION and Amazon Web Services HealthScribe starts the analytics.  You can pause or resume streaming. To pause streaming, complete the input stream without sending the MedicalScribeSessionControlEvent. To resume streaming, call the StartMedicalScribeStream and specify the same SessionId you used to start the stream.  The following parameters are required:    language-code     media-encoding     media-sample-rate-hertz     For more information on streaming with Amazon Web Services HealthScribe, see Amazon Web Services HealthScribe.
    ///
    /// Parameters:
    ///   - inputStream: Specify the input stream where you will send events in real time. The first element of the input stream must be a MedicalScribeConfigurationEvent.
    ///   - languageCode: Specify the language code for your HealthScribe streaming session.
    ///   - mediaEncoding: Specify the encoding used for the input audio. Supported formats are:   FLAC   OPUS-encoded audio in an Ogg container   PCM (only signed 16-bit little-endian audio formats, which does not include WAV)    For more information, see Media formats.
    ///   - mediaSampleRateHertz: Specify the sample rate of the input audio (in hertz). Amazon Web Services HealthScribe supports a range from 16,000 Hz to 48,000 Hz. The sample rate you specify must match that of your audio.
    ///   - sessionId: Specify an identifier for your streaming session (in UUID format). If you don't include a SessionId in your request, Amazon Web Services HealthScribe generates an ID and returns it in the response.
    ///   - logger: Logger use during operation
    @inlinable
    public func startMedicalScribeStream(
        inputStream: AWSEventStream<MedicalScribeInputStream>,
        languageCode: MedicalScribeLanguageCode,
        mediaEncoding: MedicalScribeMediaEncoding,
        mediaSampleRateHertz: Int,
        sessionId: String? = nil,
        logger: Logger = AWSClient.loggingDisabled        
    ) async throws -> StartMedicalScribeStreamResponse {
        let input = StartMedicalScribeStreamRequest(
            inputStream: inputStream, 
            languageCode: languageCode, 
            mediaEncoding: mediaEncoding, 
            mediaSampleRateHertz: mediaSampleRateHertz, 
            sessionId: sessionId
        )
        return try await self.startMedicalScribeStream(input, logger: logger)
    }

    /// Starts a bidirectional HTTP/2 or WebSocket stream where audio is streamed to  Amazon Transcribe Medical and the transcription results are streamed to your application. The following parameters are required:    language-code     media-encoding     sample-rate    For more information on streaming with Amazon Transcribe Medical, see  Transcribing streaming audio.
    @Sendable
    @inlinable
    public func startMedicalStreamTranscription(_ input: StartMedicalStreamTranscriptionRequest, logger: Logger = AWSClient.loggingDisabled) async throws -> StartMedicalStreamTranscriptionResponse {
        try await self.client.execute(
            operation: "StartMedicalStreamTranscription", 
            path: "/medical-stream-transcription", 
            httpMethod: .POST, 
            serviceConfig: self.config, 
            input: input, 
            logger: logger
        )
    }
    /// Starts a bidirectional HTTP/2 or WebSocket stream where audio is streamed to  Amazon Transcribe Medical and the transcription results are streamed to your application. The following parameters are required:    language-code     media-encoding     sample-rate    For more information on streaming with Amazon Transcribe Medical, see  Transcribing streaming audio.
    ///
    /// Parameters:
    ///   - audioStream: 
    ///   - contentIdentificationType: Labels all personal health information (PHI) identified in your transcript. Content identification is performed at the segment level; PHI is flagged upon complete transcription of an audio segment. For more information, see Identifying personal health information (PHI) in a transcription.
    ///   - enableChannelIdentification: Enables channel identification in multi-channel audio. Channel identification transcribes the audio on each channel independently, then appends the output for each channel into one transcript. If you have multi-channel audio and do not enable channel identification, your audio is  transcribed in a continuous manner and your transcript is not separated by channel. If you include EnableChannelIdentification in your request, you must also  include NumberOfChannels. For more information, see Transcribing multi-channel audio.
    ///   - languageCode: Specify the language code that represents the language spoken in your audio.  Amazon Transcribe Medical only supports US English (en-US).
    ///   - mediaEncoding: Specify the encoding used for the input audio. Supported formats are:   FLAC   OPUS-encoded audio in an Ogg container   PCM (only signed 16-bit little-endian audio formats, which does not include WAV)   For more information, see Media formats.
    ///   - mediaSampleRateHertz: The sample rate of the input audio (in hertz). Amazon Transcribe Medical supports a range from 16,000 Hz to 48,000 Hz. Note that the sample rate you specify must match that of your audio.
    ///   - numberOfChannels: Specify the number of channels in your audio stream. This value must be  2, as only two channels are supported. If your audio doesn't contain  multiple channels, do not include this parameter in your request. If you include NumberOfChannels in your request, you must also  include EnableChannelIdentification.
    ///   - sessionId: Specify a name for your transcription session. If you don't include this parameter in  your request, Amazon Transcribe Medical generates an ID and returns it in the response.
    ///   - showSpeakerLabel: Enables speaker partitioning (diarization) in your transcription output. Speaker partitioning labels the speech from individual speakers in your media file. For more information, see Partitioning speakers (diarization).
    ///   - specialty: Specify the medical specialty contained in your audio.
    ///   - type: Specify the type of input audio. For example, choose DICTATION for a  provider dictating patient notes and CONVERSATION for a dialogue between a patient and a medical professional.
    ///   - vocabularyName: Specify the name of the custom vocabulary that you want to use when processing your transcription. Note that vocabulary names are case sensitive.
    ///   - logger: Logger use during operation
    @inlinable
    public func startMedicalStreamTranscription(
        audioStream: AWSEventStream<AudioStream>,
        contentIdentificationType: MedicalContentIdentificationType? = nil,
        enableChannelIdentification: Bool? = nil,
        languageCode: LanguageCode,
        mediaEncoding: MediaEncoding,
        mediaSampleRateHertz: Int,
        numberOfChannels: Int? = nil,
        sessionId: String? = nil,
        showSpeakerLabel: Bool? = nil,
        specialty: Specialty,
        type: `Type`,
        vocabularyName: String? = nil,
        logger: Logger = AWSClient.loggingDisabled        
    ) async throws -> StartMedicalStreamTranscriptionResponse {
        let input = StartMedicalStreamTranscriptionRequest(
            audioStream: audioStream, 
            contentIdentificationType: contentIdentificationType, 
            enableChannelIdentification: enableChannelIdentification, 
            languageCode: languageCode, 
            mediaEncoding: mediaEncoding, 
            mediaSampleRateHertz: mediaSampleRateHertz, 
            numberOfChannels: numberOfChannels, 
            sessionId: sessionId, 
            showSpeakerLabel: showSpeakerLabel, 
            specialty: specialty, 
            type: type, 
            vocabularyName: vocabularyName
        )
        return try await self.startMedicalStreamTranscription(input, logger: logger)
    }

    /// Starts a bidirectional HTTP/2 or WebSocket stream where audio is streamed to  Amazon Transcribe and the transcription results are streamed to your application. The following parameters are required:    language-code or identify-language or identify-multiple-language     media-encoding     sample-rate    For more information on streaming with Amazon Transcribe, see Transcribing streaming audio.
    @Sendable
    @inlinable
    public func startStreamTranscription(_ input: StartStreamTranscriptionRequest, logger: Logger = AWSClient.loggingDisabled) async throws -> StartStreamTranscriptionResponse {
        try await self.client.execute(
            operation: "StartStreamTranscription", 
            path: "/stream-transcription", 
            httpMethod: .POST, 
            serviceConfig: self.config, 
            input: input, 
            logger: logger
        )
    }
    /// Starts a bidirectional HTTP/2 or WebSocket stream where audio is streamed to  Amazon Transcribe and the transcription results are streamed to your application. The following parameters are required:    language-code or identify-language or identify-multiple-language     media-encoding     sample-rate    For more information on streaming with Amazon Transcribe, see Transcribing streaming audio.
    ///
    /// Parameters:
    ///   - audioStream: An encoded stream of audio blobs. Audio streams are encoded as either HTTP/2 or WebSocket  data frames. For more information, see Transcribing streaming audio.
    ///   - contentIdentificationType: Labels all personally identifiable information (PII) identified in your transcript. Content identification is performed at the segment level; PII specified in  PiiEntityTypes is flagged upon complete transcription of an audio segment. If you don't include PiiEntityTypes in your request, all PII is identified. You canâ€™t set ContentIdentificationType and ContentRedactionType in the same request. If you set both, your request returns a BadRequestException. For more information, see Redacting or identifying personally identifiable information.
    ///   - contentRedactionType: Redacts all personally identifiable information (PII) identified in your transcript. Content redaction is performed at the segment level; PII specified in  PiiEntityTypes is redacted upon complete transcription of an audio segment. If you don't include PiiEntityTypes in your request, all PII is redacted. You canâ€™t set ContentRedactionType and ContentIdentificationType in the same request. If you set both, your request returns a BadRequestException. For more information, see Redacting or identifying personally identifiable information.
    ///   - enableChannelIdentification: Enables channel identification in multi-channel audio. Channel identification transcribes the audio on each channel independently, then appends the  output for each channel into one transcript. If you have multi-channel audio and do not enable channel identification, your audio is  transcribed in a continuous manner and your transcript is not separated by channel. If you include EnableChannelIdentification in your request, you must also  include NumberOfChannels. For more information, see Transcribing multi-channel audio.
    ///   - enablePartialResultsStabilization: Enables partial result stabilization for your transcription. Partial result stabilization can reduce latency in your output, but may impact accuracy. For more information, see  Partial-result  stabilization.
    ///   - identifyLanguage: Enables automatic language identification for your transcription. If you include IdentifyLanguage, you must include a list of language codes, using LanguageOptions, that you think may be present in  your audio stream.  You can also include a preferred language using PreferredLanguage. Adding a  preferred language can help Amazon Transcribe identify the language faster than if you omit this  parameter. If you have multi-channel audio that contains different languages on each channel, and you've  enabled channel identification, automatic language identification identifies the dominant language on  each audio channel. Note that you must include either LanguageCode or  IdentifyLanguage or IdentifyMultipleLanguages in your request. If you include more than one of these parameters, your transcription job fails. Streaming language identification can't be combined with custom language models or  redaction.
    ///   - identifyMultipleLanguages: Enables automatic multi-language identification in your transcription job request. Use this parameter if your stream contains more than one language. If your stream contains only one language, use IdentifyLanguage instead. If you include IdentifyMultipleLanguages, you must include a list of language codes, using LanguageOptions, that you think may be present in your stream. If you want to apply a custom vocabulary or a custom vocabulary filter to your automatic multiple language identification request, include VocabularyNames or VocabularyFilterNames. Note that you must include one of LanguageCode, IdentifyLanguage, or IdentifyMultipleLanguages in your request. If you include more than one of these parameters, your transcription job fails.
    ///   - languageCode: Specify the language code that represents the language spoken in your audio. If you're unsure of the language spoken in your audio, consider using  IdentifyLanguage to enable automatic language identification. For a list of languages supported with Amazon Transcribe streaming, refer to the  Supported  languages table.
    ///   - languageModelName: Specify the name of the custom language model that you want to use when processing your transcription. Note that language model names are case sensitive. The language of the specified language model must match the language code you specify in your transcription request. If the languages don't match, the custom language model isn't applied.  There are no errors or warnings associated with a language mismatch. For more information, see Custom language models.
    ///   - languageOptions: Specify two or more language codes that represent the languages you think may be present  in your media; including more than five is not recommended. Including language options can improve the accuracy of language identification. If you include LanguageOptions in your request, you must also include  IdentifyLanguage or IdentifyMultipleLanguages. For a list of languages supported with Amazon Transcribe streaming, refer to the  Supported  languages table.  You can only include one language dialect per language per stream. For example, you cannot include en-US and en-AU in the same request.
    ///   - mediaEncoding: Specify the encoding of your input audio. Supported formats are:   FLAC   OPUS-encoded audio in an Ogg container   PCM (only signed 16-bit little-endian audio formats, which does not include WAV)   For more information, see Media formats.
    ///   - mediaSampleRateHertz: The sample rate of the input audio (in hertz). Low-quality audio, such as telephone audio, is typically around 8,000 Hz. High-quality audio typically ranges from 16,000 Hz to 48,000 Hz. Note that the sample rate you specify must match that of your audio.
    ///   - numberOfChannels: Specify the number of channels in your audio stream. This value must be  2, as only two channels are supported. If your audio doesn't contain  multiple channels, do not include this parameter in your request. If you include NumberOfChannels in your request, you must also  include EnableChannelIdentification.
    ///   - partialResultsStability: Specify the level of stability to use when you enable partial results stabilization  (EnablePartialResultsStabilization). Low stability provides the highest accuracy. High stability transcribes faster, but with slightly lower accuracy. For more information, see Partial-result  stabilization.
    ///   - piiEntityTypes: Specify which types of personally identifiable information (PII) you want to redact in your  transcript. You can include as many types as you'd like, or you can select  ALL. Values must be comma-separated and can include: ADDRESS,  BANK_ACCOUNT_NUMBER, BANK_ROUTING, CREDIT_DEBIT_CVV, CREDIT_DEBIT_EXPIRY, CREDIT_DEBIT_NUMBER, EMAIL,  NAME, PHONE, PIN,  SSN, or ALL. Note that if you include PiiEntityTypes in your request, you must also include  ContentIdentificationType or ContentRedactionType. If you include ContentRedactionType or  ContentIdentificationType in your request, but do not include  PiiEntityTypes, all PII is redacted or identified.
    ///   - preferredLanguage: Specify a preferred language from the subset of languages codes you specified in  LanguageOptions. You can only use this parameter if you've included IdentifyLanguage and LanguageOptions in your request.
    ///   - sessionId: Specify a name for your transcription session. If you don't include this parameter in your request,  Amazon Transcribe generates an ID and returns it in the response.
    ///   - showSpeakerLabel: Enables speaker partitioning (diarization) in your transcription output. Speaker partitioning  labels the speech from individual speakers in your media file. For more information, see Partitioning speakers (diarization).
    ///   - vocabularyFilterMethod: Specify how you want your vocabulary filter applied to your transcript. To replace words with ***, choose mask. To delete words, choose remove. To flag words without changing them, choose tag.
    ///   - vocabularyFilterName: Specify the name of the custom vocabulary filter that you want to use when processing your transcription. Note that vocabulary filter names are case sensitive. If the language of the specified custom vocabulary filter doesn't match the language identified in your media, the vocabulary filter is not applied to your transcription.  This parameter is not intended for use with the IdentifyLanguage parameter. If you're including IdentifyLanguage in your request and want to use one or more vocabulary filters with your transcription, use the VocabularyFilterNames parameter instead.  For more information, see Using vocabulary filtering with unwanted  words.
    ///   - vocabularyFilterNames: Specify the names of the custom vocabulary filters that you want to use when processing your transcription. Note that vocabulary filter names are case sensitive. If none of the languages of the specified custom vocabulary filters match the language identified in your media, your job fails.  This parameter is only intended for use with  the IdentifyLanguage parameter. If you're not  including IdentifyLanguage in your request and want to use a custom vocabulary filter  with your transcription, use the VocabularyFilterName parameter instead.  For more information, see Using vocabulary filtering with unwanted  words.
    ///   - vocabularyName: Specify the name of the custom vocabulary that you want to use when processing your transcription. Note that vocabulary names are case sensitive. If the language of the specified custom vocabulary doesn't match the language identified in your media, the custom vocabulary is not applied to your transcription.  This parameter is not intended for use with the IdentifyLanguage parameter. If you're including IdentifyLanguage in your request and want to use one or more custom vocabularies with your transcription, use the VocabularyNames parameter instead.  For more information, see Custom vocabularies.
    ///   - vocabularyNames: Specify the names of the custom vocabularies that you want to use when processing your transcription. Note that vocabulary names are case sensitive. If none of the languages of the specified custom vocabularies match the language identified in  your media, your job fails.  This parameter is only intended for use with the IdentifyLanguage parameter. If you're not including IdentifyLanguage in your request and want to use a custom vocabulary with your transcription, use the VocabularyName parameter instead.  For more information, see Custom vocabularies.
    ///   - logger: Logger use during operation
    @inlinable
    public func startStreamTranscription(
        audioStream: AWSEventStream<AudioStream>,
        contentIdentificationType: ContentIdentificationType? = nil,
        contentRedactionType: ContentRedactionType? = nil,
        enableChannelIdentification: Bool? = nil,
        enablePartialResultsStabilization: Bool? = nil,
        identifyLanguage: Bool? = nil,
        identifyMultipleLanguages: Bool? = nil,
        languageCode: LanguageCode? = nil,
        languageModelName: String? = nil,
        languageOptions: String? = nil,
        mediaEncoding: MediaEncoding,
        mediaSampleRateHertz: Int,
        numberOfChannels: Int? = nil,
        partialResultsStability: PartialResultsStability? = nil,
        piiEntityTypes: String? = nil,
        preferredLanguage: LanguageCode? = nil,
        sessionId: String? = nil,
        showSpeakerLabel: Bool? = nil,
        vocabularyFilterMethod: VocabularyFilterMethod? = nil,
        vocabularyFilterName: String? = nil,
        vocabularyFilterNames: String? = nil,
        vocabularyName: String? = nil,
        vocabularyNames: String? = nil,
        logger: Logger = AWSClient.loggingDisabled        
    ) async throws -> StartStreamTranscriptionResponse {
        let input = StartStreamTranscriptionRequest(
            audioStream: audioStream, 
            contentIdentificationType: contentIdentificationType, 
            contentRedactionType: contentRedactionType, 
            enableChannelIdentification: enableChannelIdentification, 
            enablePartialResultsStabilization: enablePartialResultsStabilization, 
            identifyLanguage: identifyLanguage, 
            identifyMultipleLanguages: identifyMultipleLanguages, 
            languageCode: languageCode, 
            languageModelName: languageModelName, 
            languageOptions: languageOptions, 
            mediaEncoding: mediaEncoding, 
            mediaSampleRateHertz: mediaSampleRateHertz, 
            numberOfChannels: numberOfChannels, 
            partialResultsStability: partialResultsStability, 
            piiEntityTypes: piiEntityTypes, 
            preferredLanguage: preferredLanguage, 
            sessionId: sessionId, 
            showSpeakerLabel: showSpeakerLabel, 
            vocabularyFilterMethod: vocabularyFilterMethod, 
            vocabularyFilterName: vocabularyFilterName, 
            vocabularyFilterNames: vocabularyFilterNames, 
            vocabularyName: vocabularyName, 
            vocabularyNames: vocabularyNames
        )
        return try await self.startStreamTranscription(input, logger: logger)
    }
}

extension TranscribeStreaming {
    /// Initializer required by `AWSService.with(middlewares:timeout:byteBufferAllocator:options)`. You are not able to use this initializer directly as there are not public
    /// initializers for `AWSServiceConfig.Patch`. Please use `AWSService.with(middlewares:timeout:byteBufferAllocator:options)` instead.
    public init(from: TranscribeStreaming, patch: AWSServiceConfig.Patch) {
        self.client = from.client
        self.config = from.config.with(patch: patch)
    }
}
