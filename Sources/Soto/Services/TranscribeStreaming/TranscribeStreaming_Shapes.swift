//===----------------------------------------------------------------------===//
//
// This source file is part of the Soto for AWS open source project
//
// Copyright (c) 2017-2021 the Soto project authors
// Licensed under Apache License v2.0
//
// See LICENSE.txt for license information
// See CONTRIBUTORS.txt for the list of Soto project authors
//
// SPDX-License-Identifier: Apache-2.0
//
//===----------------------------------------------------------------------===//

// THIS FILE IS AUTOMATICALLY GENERATED by https://github.com/soto-project/soto-codegenerator.
// DO NOT EDIT.

import Foundation
import SotoCore

extension TranscribeStreaming {
    // MARK: Enums

    public enum ContentIdentificationType: String, CustomStringConvertible, Codable {
        case pii = "PII"
        public var description: String { return self.rawValue }
    }

    public enum ContentRedactionType: String, CustomStringConvertible, Codable {
        case pii = "PII"
        public var description: String { return self.rawValue }
    }

    public enum ItemType: String, CustomStringConvertible, Codable {
        case pronunciation
        case punctuation
        public var description: String { return self.rawValue }
    }

    public enum LanguageCode: String, CustomStringConvertible, Codable {
        case deDE = "de-DE"
        case enAU = "en-AU"
        case enGB = "en-GB"
        case enUS = "en-US"
        case esUS = "es-US"
        case frCA = "fr-CA"
        case frFR = "fr-FR"
        case itIT = "it-IT"
        case jaJP = "ja-JP"
        case koKR = "ko-KR"
        case ptBR = "pt-BR"
        case zhCN = "zh-CN"
        public var description: String { return self.rawValue }
    }

    public enum MediaEncoding: String, CustomStringConvertible, Codable {
        case flac
        case oggOpus = "ogg-opus"
        case pcm
        public var description: String { return self.rawValue }
    }

    public enum MedicalContentIdentificationType: String, CustomStringConvertible, Codable {
        case phi = "PHI"
        public var description: String { return self.rawValue }
    }

    public enum PartialResultsStability: String, CustomStringConvertible, Codable {
        case high
        case low
        case medium
        public var description: String { return self.rawValue }
    }

    public enum Specialty: String, CustomStringConvertible, Codable {
        case cardiology = "CARDIOLOGY"
        case neurology = "NEUROLOGY"
        case oncology = "ONCOLOGY"
        case primarycare = "PRIMARYCARE"
        case radiology = "RADIOLOGY"
        case urology = "UROLOGY"
        public var description: String { return self.rawValue }
    }

    public enum `Type`: String, CustomStringConvertible, Codable {
        case conversation = "CONVERSATION"
        case dictation = "DICTATION"
        public var description: String { return self.rawValue }
    }

    public enum VocabularyFilterMethod: String, CustomStringConvertible, Codable {
        case mask
        case remove
        case tag
        public var description: String { return self.rawValue }
    }

    public enum MedicalTranscriptResultStream: AWSDecodableShape {
        case badRequestException(BadRequestException)
        case conflictException(ConflictException)
        case internalFailureException(InternalFailureException)
        case limitExceededException(LimitExceededException)
        case serviceUnavailableException(ServiceUnavailableException)
        /// A portion of the transcription of the audio stream. Events are sent periodically from Amazon Transcribe Medical to your application. The event can be a partial transcription of a section of the audio stream, or it can be the entire transcription of that portion of the audio stream.
        case transcriptEvent(MedicalTranscriptEvent)

        public init(from decoder: Decoder) throws {
            let container = try decoder.container(keyedBy: CodingKeys.self)
            guard container.allKeys.count == 1, let key = container.allKeys.first else {
                let context = DecodingError.Context(
                    codingPath: container.codingPath,
                    debugDescription: "Expected exactly one key, but got \(container.allKeys.count)"
                )
                throw DecodingError.dataCorrupted(context)
            }
            switch key {
            case .badRequestException:
                let value = try container.decode(BadRequestException.self, forKey: .badRequestException)
                self = .badRequestException(value)
            case .conflictException:
                let value = try container.decode(ConflictException.self, forKey: .conflictException)
                self = .conflictException(value)
            case .internalFailureException:
                let value = try container.decode(InternalFailureException.self, forKey: .internalFailureException)
                self = .internalFailureException(value)
            case .limitExceededException:
                let value = try container.decode(LimitExceededException.self, forKey: .limitExceededException)
                self = .limitExceededException(value)
            case .serviceUnavailableException:
                let value = try container.decode(ServiceUnavailableException.self, forKey: .serviceUnavailableException)
                self = .serviceUnavailableException(value)
            case .transcriptEvent:
                let value = try container.decode(MedicalTranscriptEvent.self, forKey: .transcriptEvent)
                self = .transcriptEvent(value)
            }
        }

        private enum CodingKeys: String, CodingKey {
            case badRequestException = "BadRequestException"
            case conflictException = "ConflictException"
            case internalFailureException = "InternalFailureException"
            case limitExceededException = "LimitExceededException"
            case serviceUnavailableException = "ServiceUnavailableException"
            case transcriptEvent = "TranscriptEvent"
        }
    }

    public enum TranscriptResultStream: AWSDecodableShape {
        /// A client error occurred when the stream was created. Check the parameters of the request and try your request again.
        case badRequestException(BadRequestException)
        /// A new stream started with the same session ID. The current stream has been terminated.
        case conflictException(ConflictException)
        /// A problem occurred while processing the audio. Amazon Transcribe terminated processing.
        case internalFailureException(InternalFailureException)
        /// Your client has exceeded one of the Amazon Transcribe limits, typically the limit on audio length. Break your audio stream into smaller chunks and try your request again.
        case limitExceededException(LimitExceededException)
        /// Service is currently unavailable. Try your request later.
        case serviceUnavailableException(ServiceUnavailableException)
        /// A portion of the transcription of the audio stream. Events are sent periodically from Amazon Transcribe to your application. The event can be a partial transcription of a section of the audio stream, or it can be the entire transcription of that portion of the audio stream.
        case transcriptEvent(TranscriptEvent)

        public init(from decoder: Decoder) throws {
            let container = try decoder.container(keyedBy: CodingKeys.self)
            guard container.allKeys.count == 1, let key = container.allKeys.first else {
                let context = DecodingError.Context(
                    codingPath: container.codingPath,
                    debugDescription: "Expected exactly one key, but got \(container.allKeys.count)"
                )
                throw DecodingError.dataCorrupted(context)
            }
            switch key {
            case .badRequestException:
                let value = try container.decode(BadRequestException.self, forKey: .badRequestException)
                self = .badRequestException(value)
            case .conflictException:
                let value = try container.decode(ConflictException.self, forKey: .conflictException)
                self = .conflictException(value)
            case .internalFailureException:
                let value = try container.decode(InternalFailureException.self, forKey: .internalFailureException)
                self = .internalFailureException(value)
            case .limitExceededException:
                let value = try container.decode(LimitExceededException.self, forKey: .limitExceededException)
                self = .limitExceededException(value)
            case .serviceUnavailableException:
                let value = try container.decode(ServiceUnavailableException.self, forKey: .serviceUnavailableException)
                self = .serviceUnavailableException(value)
            case .transcriptEvent:
                let value = try container.decode(TranscriptEvent.self, forKey: .transcriptEvent)
                self = .transcriptEvent(value)
            }
        }

        private enum CodingKeys: String, CodingKey {
            case badRequestException = "BadRequestException"
            case conflictException = "ConflictException"
            case internalFailureException = "InternalFailureException"
            case limitExceededException = "LimitExceededException"
            case serviceUnavailableException = "ServiceUnavailableException"
            case transcriptEvent = "TranscriptEvent"
        }
    }

    // MARK: Shapes

    public struct Alternative: AWSDecodableShape {
        /// Contains the entities identified as personally identifiable information (PII) in the transcription output.
        public let entities: [Entity]?
        /// One or more alternative interpretations of the input audio.
        public let items: [Item]?
        /// The text that was transcribed from the audio.
        public let transcript: String?

        public init(entities: [Entity]? = nil, items: [Item]? = nil, transcript: String? = nil) {
            self.entities = entities
            self.items = items
            self.transcript = transcript
        }

        private enum CodingKeys: String, CodingKey {
            case entities = "Entities"
            case items = "Items"
            case transcript = "Transcript"
        }
    }

    public struct AudioEvent: AWSEncodableShape {
        /// An audio blob that contains the next part of the audio that you want to transcribe. The maximum audio chunk size is 32 KB.
        public let audioChunk: Data?

        public init(audioChunk: Data? = nil) {
            self.audioChunk = audioChunk
        }

        private enum CodingKeys: String, CodingKey {
            case audioChunk = "AudioChunk"
        }
    }

    public struct BadRequestException: AWSDecodableShape {
        public let message: String?

        public init(message: String? = nil) {
            self.message = message
        }

        private enum CodingKeys: String, CodingKey {
            case message = "Message"
        }
    }

    public struct ConflictException: AWSDecodableShape {
        public let message: String?

        public init(message: String? = nil) {
            self.message = message
        }

        private enum CodingKeys: String, CodingKey {
            case message = "Message"
        }
    }

    public struct Entity: AWSDecodableShape {
        /// The category of information identified in this entity; for example, PII.
        public let category: String?
        /// A value between zero and one that Amazon Transcribe assigns to PII identified in the source audio. Larger values indicate a higher confidence in PII identification.
        public let confidence: Double?
        /// The words in the transcription output that have been identified as a PII entity.
        public let content: String?
        /// The end time of speech that was identified as PII.
        public let endTime: Double?
        /// The start time of speech that was identified as PII.
        public let startTime: Double?
        /// The type of PII identified in this entity; for example, name or credit card number.
        public let type: String?

        public init(category: String? = nil, confidence: Double? = nil, content: String? = nil, endTime: Double? = nil, startTime: Double? = nil, type: String? = nil) {
            self.category = category
            self.confidence = confidence
            self.content = content
            self.endTime = endTime
            self.startTime = startTime
            self.type = type
        }

        private enum CodingKeys: String, CodingKey {
            case category = "Category"
            case confidence = "Confidence"
            case content = "Content"
            case endTime = "EndTime"
            case startTime = "StartTime"
            case type = "Type"
        }
    }

    public struct InternalFailureException: AWSDecodableShape {
        public let message: String?

        public init(message: String? = nil) {
            self.message = message
        }

        private enum CodingKeys: String, CodingKey {
            case message = "Message"
        }
    }

    public struct Item: AWSDecodableShape {
        /// A value between zero and one for an item that is a confidence score that Amazon Transcribe assigns to each word or phrase that it transcribes.
        public let confidence: Double?
        /// The word or punctuation that was recognized in the input audio.
        public let content: String?
        /// The offset from the beginning of the audio stream to the end of the audio that resulted in the item.
        public let endTime: Double?
        /// If speaker identification is enabled, shows the speakers identified in the media stream.
        public let speaker: String?
        /// If partial result stabilization has been enabled, indicates whether the word or phrase in the item is stable. If Stable is true, the result is stable.
        public let stable: Bool?
        /// The offset from the beginning of the audio stream to the beginning of the audio that resulted in the item.
        public let startTime: Double?
        /// The type of the item. PRONUNCIATION indicates that the item is a word that was recognized in the input audio. PUNCTUATION indicates that the item was interpreted as a pause in the input audio.
        public let type: ItemType?
        /// Indicates whether a word in the item matches a word in the vocabulary filter you've chosen for your media stream. If true then a word in the item matches your vocabulary filter.
        public let vocabularyFilterMatch: Bool?

        public init(confidence: Double? = nil, content: String? = nil, endTime: Double? = nil, speaker: String? = nil, stable: Bool? = nil, startTime: Double? = nil, type: ItemType? = nil, vocabularyFilterMatch: Bool? = nil) {
            self.confidence = confidence
            self.content = content
            self.endTime = endTime
            self.speaker = speaker
            self.stable = stable
            self.startTime = startTime
            self.type = type
            self.vocabularyFilterMatch = vocabularyFilterMatch
        }

        private enum CodingKeys: String, CodingKey {
            case confidence = "Confidence"
            case content = "Content"
            case endTime = "EndTime"
            case speaker = "Speaker"
            case stable = "Stable"
            case startTime = "StartTime"
            case type = "Type"
            case vocabularyFilterMatch = "VocabularyFilterMatch"
        }
    }

    public struct LanguageWithScore: AWSDecodableShape {
        /// The language code of the language identified by Amazon Transcribe.
        public let languageCode: LanguageCode?
        /// The confidence score for the associated language code. Confidence scores are values between zero and one; larger values indicate a higher confidence in the identified language.
        public let score: Double?

        public init(languageCode: LanguageCode? = nil, score: Double? = nil) {
            self.languageCode = languageCode
            self.score = score
        }

        private enum CodingKeys: String, CodingKey {
            case languageCode = "LanguageCode"
            case score = "Score"
        }
    }

    public struct LimitExceededException: AWSDecodableShape {
        public let message: String?

        public init(message: String? = nil) {
            self.message = message
        }

        private enum CodingKeys: String, CodingKey {
            case message = "Message"
        }
    }

    public struct MedicalAlternative: AWSDecodableShape {
        /// Contains the medical entities identified as personal health information in the transcription output.
        public let entities: [MedicalEntity]?
        /// A list of objects that contains words and punctuation marks that represents one or more interpretations of the input audio.
        public let items: [MedicalItem]?
        /// The text that was transcribed from the audio.
        public let transcript: String?

        public init(entities: [MedicalEntity]? = nil, items: [MedicalItem]? = nil, transcript: String? = nil) {
            self.entities = entities
            self.items = items
            self.transcript = transcript
        }

        private enum CodingKeys: String, CodingKey {
            case entities = "Entities"
            case items = "Items"
            case transcript = "Transcript"
        }
    }

    public struct MedicalEntity: AWSDecodableShape {
        /// The type of personal health information of the medical entity.
        public let category: String?
        /// A value between zero and one that Amazon Transcribe Medical assigned to the personal health information that it identified in the source audio. Larger values indicate that Amazon Transcribe Medical has higher confidence in the personal health information that it identified.
        public let confidence: Double?
        /// The word or words in the transcription output that have been identified as a medical entity.
        public let content: String?
        /// The end time of the speech that was identified as a medical entity.
        public let endTime: Double?
        /// The start time of the speech that was identified as a medical entity.
        public let startTime: Double?

        public init(category: String? = nil, confidence: Double? = nil, content: String? = nil, endTime: Double? = nil, startTime: Double? = nil) {
            self.category = category
            self.confidence = confidence
            self.content = content
            self.endTime = endTime
            self.startTime = startTime
        }

        private enum CodingKeys: String, CodingKey {
            case category = "Category"
            case confidence = "Confidence"
            case content = "Content"
            case endTime = "EndTime"
            case startTime = "StartTime"
        }
    }

    public struct MedicalItem: AWSDecodableShape {
        /// A value between 0 and 1 for an item that is a confidence score that Amazon Transcribe Medical assigns to each word that it transcribes.
        public let confidence: Double?
        /// The word or punctuation mark that was recognized in the input audio.
        public let content: String?
        /// The number of seconds into an audio stream that indicates the creation time of an item.
        public let endTime: Double?
        /// If speaker identification is enabled, shows the integer values that correspond to the different speakers identified in the stream. For example, if the value of Speaker in the stream is either a 0 or a 1, that indicates that Amazon Transcribe Medical has identified two speakers in the stream. The value of 0 corresponds to one speaker and the value of 1 corresponds to the other speaker.
        public let speaker: String?
        /// The number of seconds into an audio stream that indicates the creation time of an item.
        public let startTime: Double?
        /// The type of the item. PRONUNCIATION indicates that the item is a word that was recognized in the input audio. PUNCTUATION indicates that the item was interpreted as a pause in the input audio, such as a period to indicate the end of a sentence.
        public let type: ItemType?

        public init(confidence: Double? = nil, content: String? = nil, endTime: Double? = nil, speaker: String? = nil, startTime: Double? = nil, type: ItemType? = nil) {
            self.confidence = confidence
            self.content = content
            self.endTime = endTime
            self.speaker = speaker
            self.startTime = startTime
            self.type = type
        }

        private enum CodingKeys: String, CodingKey {
            case confidence = "Confidence"
            case content = "Content"
            case endTime = "EndTime"
            case speaker = "Speaker"
            case startTime = "StartTime"
            case type = "Type"
        }
    }

    public struct MedicalResult: AWSDecodableShape {
        /// A list of possible transcriptions of the audio. Each alternative typically contains one Item that contains the result of the transcription.
        public let alternatives: [MedicalAlternative]?
        /// When channel identification is enabled, Amazon Transcribe Medical transcribes the speech from each audio channel separately. You can use ChannelId to retrieve the transcription results for a single channel in your audio stream.
        public let channelId: String?
        /// The time, in seconds, from the beginning of the audio stream to the end of the result.
        public let endTime: Double?
        /// Amazon Transcribe Medical divides the incoming audio stream into segments at natural points in the audio. Transcription results are returned based on these segments. The IsPartial field is true to indicate that Amazon Transcribe Medical has additional transcription data to send. The IsPartial field is false to indicate that this is the last transcription result for the segment.
        public let isPartial: Bool?
        /// A unique identifier for the result.
        public let resultId: String?
        /// The time, in seconds, from the beginning of the audio stream to the beginning of the result.
        public let startTime: Double?

        public init(alternatives: [MedicalAlternative]? = nil, channelId: String? = nil, endTime: Double? = nil, isPartial: Bool? = nil, resultId: String? = nil, startTime: Double? = nil) {
            self.alternatives = alternatives
            self.channelId = channelId
            self.endTime = endTime
            self.isPartial = isPartial
            self.resultId = resultId
            self.startTime = startTime
        }

        private enum CodingKeys: String, CodingKey {
            case alternatives = "Alternatives"
            case channelId = "ChannelId"
            case endTime = "EndTime"
            case isPartial = "IsPartial"
            case resultId = "ResultId"
            case startTime = "StartTime"
        }
    }

    public struct MedicalTranscript: AWSDecodableShape {
        ///  MedicalResult objects that contain the results of transcribing a portion of the input audio stream. The array can be empty.
        public let results: [MedicalResult]?

        public init(results: [MedicalResult]? = nil) {
            self.results = results
        }

        private enum CodingKeys: String, CodingKey {
            case results = "Results"
        }
    }

    public struct MedicalTranscriptEvent: AWSDecodableShape {
        /// The transcription of the audio stream. The transcription is composed of all of the items in the results list.
        public let transcript: MedicalTranscript?

        public init(transcript: MedicalTranscript? = nil) {
            self.transcript = transcript
        }

        private enum CodingKeys: String, CodingKey {
            case transcript = "Transcript"
        }
    }

    public struct Result: AWSDecodableShape {
        /// A list of possible transcriptions for the audio. Each alternative typically contains one item that contains the result of the transcription.
        public let alternatives: [Alternative]?
        /// When channel identification is enabled, Amazon Transcribe transcribes the speech from each audio channel separately. You can use ChannelId to retrieve the transcription results for a single channel in your audio stream.
        public let channelId: String?
        /// The offset in seconds from the beginning of the audio stream to the end of the result.
        public let endTime: Double?
        /// Amazon Transcribe divides the incoming audio stream into segments at natural points in the audio. Transcription results are returned based on these segments.  The IsPartial field is true to indicate that Amazon Transcribe has additional transcription data to send, false to indicate that this is the last transcription result for the segment.
        public let isPartial: Bool?
        /// The language code of the identified language in your media stream.
        public let languageCode: LanguageCode?
        /// The language code of the dominant language identified in your media.
        public let languageIdentification: [LanguageWithScore]?
        /// A unique identifier for the result.
        public let resultId: String?
        /// The offset in seconds from the beginning of the audio stream to the beginning of the result.
        public let startTime: Double?

        public init(alternatives: [Alternative]? = nil, channelId: String? = nil, endTime: Double? = nil, isPartial: Bool? = nil, languageCode: LanguageCode? = nil, languageIdentification: [LanguageWithScore]? = nil, resultId: String? = nil, startTime: Double? = nil) {
            self.alternatives = alternatives
            self.channelId = channelId
            self.endTime = endTime
            self.isPartial = isPartial
            self.languageCode = languageCode
            self.languageIdentification = languageIdentification
            self.resultId = resultId
            self.startTime = startTime
        }

        private enum CodingKeys: String, CodingKey {
            case alternatives = "Alternatives"
            case channelId = "ChannelId"
            case endTime = "EndTime"
            case isPartial = "IsPartial"
            case languageCode = "LanguageCode"
            case languageIdentification = "LanguageIdentification"
            case resultId = "ResultId"
            case startTime = "StartTime"
        }
    }

    public struct ServiceUnavailableException: AWSDecodableShape {
        public let message: String?

        public init(message: String? = nil) {
            self.message = message
        }

        private enum CodingKeys: String, CodingKey {
            case message = "Message"
        }
    }

    public struct StartMedicalStreamTranscriptionRequest: AWSEncodableShape & AWSShapeWithPayload {
        /// The key for the payload
        public static let _payloadPath: String = "audioStream"
        public static var _encoding = [
            AWSMemberEncoding(label: "audioStream", location: .body("AudioStream")),
            AWSMemberEncoding(label: "contentIdentificationType", location: .header("x-amzn-transcribe-content-identification-type")),
            AWSMemberEncoding(label: "enableChannelIdentification", location: .header("x-amzn-transcribe-enable-channel-identification")),
            AWSMemberEncoding(label: "languageCode", location: .header("x-amzn-transcribe-language-code")),
            AWSMemberEncoding(label: "mediaEncoding", location: .header("x-amzn-transcribe-media-encoding")),
            AWSMemberEncoding(label: "mediaSampleRateHertz", location: .header("x-amzn-transcribe-sample-rate")),
            AWSMemberEncoding(label: "numberOfChannels", location: .header("x-amzn-transcribe-number-of-channels")),
            AWSMemberEncoding(label: "sessionId", location: .header("x-amzn-transcribe-session-id")),
            AWSMemberEncoding(label: "showSpeakerLabel", location: .header("x-amzn-transcribe-show-speaker-label")),
            AWSMemberEncoding(label: "specialty", location: .header("x-amzn-transcribe-specialty")),
            AWSMemberEncoding(label: "type", location: .header("x-amzn-transcribe-type")),
            AWSMemberEncoding(label: "vocabularyName", location: .header("x-amzn-transcribe-vocabulary-name"))
        ]

        public let audioStream: AudioStream
        /// Set this field to PHI to identify personal health information in the transcription output.
        public let contentIdentificationType: MedicalContentIdentificationType?
        /// When true, instructs Amazon Transcribe Medical to process each audio channel separately and then merge the transcription output of each channel into a single transcription. Amazon Transcribe Medical also produces a transcription of each item. An item includes the start time, end time, and any alternative transcriptions. You can't set both ShowSpeakerLabel and EnableChannelIdentification in the same request. If you set both, your request returns a BadRequestException.
        public let enableChannelIdentification: Bool?
        ///  Indicates the source language used in the input audio stream. For Amazon Transcribe Medical, this is US English (en-US).
        public let languageCode: LanguageCode
        /// The encoding used for the input audio.
        public let mediaEncoding: MediaEncoding
        /// The sample rate of the input audio (in Hertz). Amazon Transcribe medical supports a range from  16,000 Hz to 48,000 Hz. Note that the sample rate you specify must match that of your audio.
        public let mediaSampleRateHertz: Int
        /// The number of channels that are in your audio stream.
        public let numberOfChannels: Int?
        ///  Optional. An identifier for the transcription session. If you don't provide a session ID, Amazon Transcribe generates one for you and returns it in the response.
        public let sessionId: String?
        /// When true, enables speaker identification in your real-time stream.
        public let showSpeakerLabel: Bool?
        /// The medical specialty of the clinician or provider.
        public let specialty: Specialty
        /// The type of input audio. Choose DICTATION for a provider dictating patient notes. Choose CONVERSATION for a dialogue between a patient and one or more medical professionanls.
        public let type: `Type`
        /// The name of the medical custom vocabulary to use when processing the real-time stream.
        public let vocabularyName: String?

        public init(audioStream: AudioStream, contentIdentificationType: MedicalContentIdentificationType? = nil, enableChannelIdentification: Bool? = nil, languageCode: LanguageCode, mediaEncoding: MediaEncoding, mediaSampleRateHertz: Int, numberOfChannels: Int? = nil, sessionId: String? = nil, showSpeakerLabel: Bool? = nil, specialty: Specialty, type: `Type`, vocabularyName: String? = nil) {
            self.audioStream = audioStream
            self.contentIdentificationType = contentIdentificationType
            self.enableChannelIdentification = enableChannelIdentification
            self.languageCode = languageCode
            self.mediaEncoding = mediaEncoding
            self.mediaSampleRateHertz = mediaSampleRateHertz
            self.numberOfChannels = numberOfChannels
            self.sessionId = sessionId
            self.showSpeakerLabel = showSpeakerLabel
            self.specialty = specialty
            self.type = type
            self.vocabularyName = vocabularyName
        }

        public func validate(name: String) throws {
            try self.validate(self.mediaSampleRateHertz, name: "mediaSampleRateHertz", parent: name, max: 48000)
            try self.validate(self.mediaSampleRateHertz, name: "mediaSampleRateHertz", parent: name, min: 8000)
            try self.validate(self.numberOfChannels, name: "numberOfChannels", parent: name, min: 2)
            try self.validate(self.sessionId, name: "sessionId", parent: name, max: 36)
            try self.validate(self.sessionId, name: "sessionId", parent: name, min: 36)
            try self.validate(self.sessionId, name: "sessionId", parent: name, pattern: "^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$")
            try self.validate(self.vocabularyName, name: "vocabularyName", parent: name, max: 200)
            try self.validate(self.vocabularyName, name: "vocabularyName", parent: name, min: 1)
            try self.validate(self.vocabularyName, name: "vocabularyName", parent: name, pattern: "^[0-9a-zA-Z._-]+$")
        }

        private enum CodingKeys: String, CodingKey {
            case audioStream = "AudioStream"
        }
    }

    public struct StartMedicalStreamTranscriptionResponse: AWSDecodableShape & AWSShapeWithPayload {
        /// The key for the payload
        public static let _payloadPath: String = "transcriptResultStream"
        public static var _encoding = [
            AWSMemberEncoding(label: "contentIdentificationType", location: .header("x-amzn-transcribe-content-identification-type")),
            AWSMemberEncoding(label: "enableChannelIdentification", location: .header("x-amzn-transcribe-enable-channel-identification")),
            AWSMemberEncoding(label: "languageCode", location: .header("x-amzn-transcribe-language-code")),
            AWSMemberEncoding(label: "mediaEncoding", location: .header("x-amzn-transcribe-media-encoding")),
            AWSMemberEncoding(label: "mediaSampleRateHertz", location: .header("x-amzn-transcribe-sample-rate")),
            AWSMemberEncoding(label: "numberOfChannels", location: .header("x-amzn-transcribe-number-of-channels")),
            AWSMemberEncoding(label: "requestId", location: .header("x-amzn-request-id")),
            AWSMemberEncoding(label: "sessionId", location: .header("x-amzn-transcribe-session-id")),
            AWSMemberEncoding(label: "showSpeakerLabel", location: .header("x-amzn-transcribe-show-speaker-label")),
            AWSMemberEncoding(label: "specialty", location: .header("x-amzn-transcribe-specialty")),
            AWSMemberEncoding(label: "transcriptResultStream", location: .body("TranscriptResultStream")),
            AWSMemberEncoding(label: "type", location: .header("x-amzn-transcribe-type")),
            AWSMemberEncoding(label: "vocabularyName", location: .header("x-amzn-transcribe-vocabulary-name"))
        ]

        /// If the value is PHI, indicates that you've configured your stream to identify personal health information.
        public let contentIdentificationType: MedicalContentIdentificationType?
        /// Shows whether channel identification has been enabled in the stream.
        public let enableChannelIdentification: Bool?
        /// The language code for the response transcript. For Amazon Transcribe Medical, this is US English (en-US).
        public let languageCode: LanguageCode?
        /// The encoding used for the input audio stream.
        public let mediaEncoding: MediaEncoding?
        /// The sample rate of the input audio, in Hertz (Hz).
        public let mediaSampleRateHertz: Int?
        /// The number of channels identified in the stream.
        public let numberOfChannels: Int?
        /// An identifier for the streaming transcription.
        public let requestId: String?
        /// Optional. An identifier for the transcription session. If you don't provide a session ID, Amazon Transcribe generates one for you and returns it in the response.
        public let sessionId: String?
        /// Shows whether speaker identification was enabled in the stream.
        public let showSpeakerLabel: Bool?
        /// The specialty in the medical domain.
        public let specialty: Specialty?
        /// Represents the stream of transcription events from Amazon Transcribe Medical to your application.
        public let transcriptResultStream: MedicalTranscriptResultStream?
        /// The type of audio that was transcribed.
        public let type: `Type`?
        /// The name of the vocabulary used when processing the stream.
        public let vocabularyName: String?

        public init(contentIdentificationType: MedicalContentIdentificationType? = nil, enableChannelIdentification: Bool? = nil, languageCode: LanguageCode? = nil, mediaEncoding: MediaEncoding? = nil, mediaSampleRateHertz: Int? = nil, numberOfChannels: Int? = nil, requestId: String? = nil, sessionId: String? = nil, showSpeakerLabel: Bool? = nil, specialty: Specialty? = nil, transcriptResultStream: MedicalTranscriptResultStream? = nil, type: `Type`? = nil, vocabularyName: String? = nil) {
            self.contentIdentificationType = contentIdentificationType
            self.enableChannelIdentification = enableChannelIdentification
            self.languageCode = languageCode
            self.mediaEncoding = mediaEncoding
            self.mediaSampleRateHertz = mediaSampleRateHertz
            self.numberOfChannels = numberOfChannels
            self.requestId = requestId
            self.sessionId = sessionId
            self.showSpeakerLabel = showSpeakerLabel
            self.specialty = specialty
            self.transcriptResultStream = transcriptResultStream
            self.type = type
            self.vocabularyName = vocabularyName
        }

        private enum CodingKeys: String, CodingKey {
            case contentIdentificationType = "x-amzn-transcribe-content-identification-type"
            case enableChannelIdentification = "x-amzn-transcribe-enable-channel-identification"
            case languageCode = "x-amzn-transcribe-language-code"
            case mediaEncoding = "x-amzn-transcribe-media-encoding"
            case mediaSampleRateHertz = "x-amzn-transcribe-sample-rate"
            case numberOfChannels = "x-amzn-transcribe-number-of-channels"
            case requestId = "x-amzn-request-id"
            case sessionId = "x-amzn-transcribe-session-id"
            case showSpeakerLabel = "x-amzn-transcribe-show-speaker-label"
            case specialty = "x-amzn-transcribe-specialty"
            case transcriptResultStream = "TranscriptResultStream"
            case type = "x-amzn-transcribe-type"
            case vocabularyName = "x-amzn-transcribe-vocabulary-name"
        }
    }

    public struct StartStreamTranscriptionRequest: AWSEncodableShape & AWSShapeWithPayload {
        /// The key for the payload
        public static let _payloadPath: String = "audioStream"
        public static var _encoding = [
            AWSMemberEncoding(label: "audioStream", location: .body("AudioStream")),
            AWSMemberEncoding(label: "contentIdentificationType", location: .header("x-amzn-transcribe-content-identification-type")),
            AWSMemberEncoding(label: "contentRedactionType", location: .header("x-amzn-transcribe-content-redaction-type")),
            AWSMemberEncoding(label: "enableChannelIdentification", location: .header("x-amzn-transcribe-enable-channel-identification")),
            AWSMemberEncoding(label: "enablePartialResultsStabilization", location: .header("x-amzn-transcribe-enable-partial-results-stabilization")),
            AWSMemberEncoding(label: "identifyLanguage", location: .header("x-amzn-transcribe-identify-language")),
            AWSMemberEncoding(label: "languageCode", location: .header("x-amzn-transcribe-language-code")),
            AWSMemberEncoding(label: "languageModelName", location: .header("x-amzn-transcribe-language-model-name")),
            AWSMemberEncoding(label: "languageOptions", location: .header("x-amzn-transcribe-language-options")),
            AWSMemberEncoding(label: "mediaEncoding", location: .header("x-amzn-transcribe-media-encoding")),
            AWSMemberEncoding(label: "mediaSampleRateHertz", location: .header("x-amzn-transcribe-sample-rate")),
            AWSMemberEncoding(label: "numberOfChannels", location: .header("x-amzn-transcribe-number-of-channels")),
            AWSMemberEncoding(label: "partialResultsStability", location: .header("x-amzn-transcribe-partial-results-stability")),
            AWSMemberEncoding(label: "piiEntityTypes", location: .header("x-amzn-transcribe-pii-entity-types")),
            AWSMemberEncoding(label: "preferredLanguage", location: .header("x-amzn-transcribe-preferred-language")),
            AWSMemberEncoding(label: "sessionId", location: .header("x-amzn-transcribe-session-id")),
            AWSMemberEncoding(label: "showSpeakerLabel", location: .header("x-amzn-transcribe-show-speaker-label")),
            AWSMemberEncoding(label: "vocabularyFilterMethod", location: .header("x-amzn-transcribe-vocabulary-filter-method")),
            AWSMemberEncoding(label: "vocabularyFilterName", location: .header("x-amzn-transcribe-vocabulary-filter-name")),
            AWSMemberEncoding(label: "vocabularyFilterNames", location: .header("x-amzn-transcribe-vocabulary-filter-names")),
            AWSMemberEncoding(label: "vocabularyName", location: .header("x-amzn-transcribe-vocabulary-name")),
            AWSMemberEncoding(label: "vocabularyNames", location: .header("x-amzn-transcribe-vocabulary-names"))
        ]

        /// PCM-encoded stream of audio blobs. The audio stream is encoded as an HTTP/2 data frame.
        public let audioStream: AudioStream
        /// Set this field to PII to identify personally identifiable information (PII) in the transcription output. Content identification is performed only upon complete transcription of the audio segments.  You can’t set both ContentIdentificationType and ContentRedactionType in the same request. If you set both, your request  returns a BadRequestException.
        public let contentIdentificationType: ContentIdentificationType?
        /// Set this field to PII to redact personally identifiable information (PII) in the transcription output. Content redaction is performed only upon complete transcription of the audio segments.  You can’t set both ContentRedactionType and ContentIdentificationType in the same request. If you set both, your request returns a BadRequestException.
        public let contentRedactionType: ContentRedactionType?
        /// When true, instructs Amazon Transcribe to process each audio channel separately, then merges the transcription output of each channel into a single transcription. Amazon Transcribe also produces a transcription of each item. An item includes the start time, end time, and any alternative transcriptions.
        public let enableChannelIdentification: Bool?
        /// When true, instructs Amazon Transcribe to present transcription results that have the partial results stabilized. Normally, any word or phrase from one partial result can change in a subsequent partial result. With partial results stabilization enabled, only the last few words of one partial result can change in another partial result.
        public let enablePartialResultsStabilization: Bool?
        /// Optional. Set this value to true to enable language identification for  your media stream.
        public let identifyLanguage: Bool?
        /// The language code of the input audio stream.
        public let languageCode: LanguageCode?
        /// The name of the language model you want to use.
        public let languageModelName: String?
        /// An object containing a list of languages that might be present in your audio. You must provide two or more language codes to help Amazon Transcribe identify the correct  language of your media stream with the highest possible accuracy. You can only select one variant per language; for example, you can't include both en-US and en-UK in the same request. You can only use this parameter if you've set IdentifyLanguage to truein your request.
        public let languageOptions: String?
        /// The encoding used for the input audio.
        public let mediaEncoding: MediaEncoding
        /// The sample rate of the input audio (in Hertz). Low-quality audio, such as telephone  audio, is typically around 8,000 Hz. High-quality audio typically ranges from 16,000 Hz to  48,000 Hz. Note that the sample rate you specify must match that of your audio.
        public let mediaSampleRateHertz: Int
        /// The number of channels that are in your audio stream.
        public let numberOfChannels: Int?
        /// You can use this field to set the stability level of the transcription results. A higher stability level means that the transcription results are less likely to change. Higher stability levels can come with lower overall transcription accuracy.
        public let partialResultsStability: PartialResultsStability?
        /// List the PII entity types you want to identify or redact. In order to specify entity types, you must have either ContentIdentificationType or ContentRedactionType enabled.  PIIEntityTypes must be comma-separated; the available values are: BANK_ACCOUNT_NUMBER, BANK_ROUTING, CREDIT_DEBIT_NUMBER, CREDIT_DEBIT_CVV,  CREDIT_DEBIT_EXPIRY, PIN, EMAIL,  ADDRESS, NAME, PHONE,  SSN, and ALL.  PiiEntityTypes is an optional parameter with a default value of ALL.
        public let piiEntityTypes: String?
        /// Optional. From the subset of languages codes you provided for  LanguageOptions, you can select one preferred language for your  transcription. You can only use this parameter if you've set IdentifyLanguage to truein your request.
        public let preferredLanguage: LanguageCode?
        /// A identifier for the transcription session. Use this parameter when you want to retry a session. If you don't provide a session ID, Amazon Transcribe will generate one for you and return it in the response.
        public let sessionId: String?
        /// When true, enables speaker identification in your media stream.
        public let showSpeakerLabel: Bool?
        /// The manner in which you use your vocabulary filter to filter words in your transcript. Remove removes filtered words from your transcription results. Mask masks filtered words with a *** in your transcription results. Tag keeps the filtered words in your transcription results and tags  them. The tag appears as VocabularyFilterMatch equal to  True.
        public let vocabularyFilterMethod: VocabularyFilterMethod?
        /// The name of the vocabulary filter you want to use with your transcription. This operation is not intended for use in conjunction with the  IdentifyLanguage operation. If you're using IdentifyLanguage in your request and want to use one or more vocabulary filters with your transcription, use the  VocabularyFilterNames operation instead.
        public let vocabularyFilterName: String?
        /// The names of the vocabulary filters you want to use with your transcription. Note that if the vocabulary filters you specify are in languages that don't match the  language identified in your media, your job fails. This operation is only intended for use in conjunction with the  IdentifyLanguage operation. If you're not using IdentifyLanguage in your request and want to use a vocabulary filter with your transcription, use the  VocabularyFilterName operation instead.
        public let vocabularyFilterNames: String?
        /// The name of the custom vocabulary you want to use with your transcription. This operation is not intended for use in conjunction with the  IdentifyLanguage operation. If you're using IdentifyLanguage in your request and want to use one or more custom vocabularies with your transcription, use the  VocabularyNames operation instead.
        public let vocabularyName: String?
        /// The names of the custom vocabularies you want to use with your transcription. Note that if the custom vocabularies you specify are in languages that don't match the  language identified in your media, your job fails. This operation is only intended for use in conjunction with the  IdentifyLanguage operation. If you're not using IdentifyLanguage in your request and want to use a custom vocabulary with your transcription, use the  VocabularyName operation instead.
        public let vocabularyNames: String?

        public init(audioStream: AudioStream, contentIdentificationType: ContentIdentificationType? = nil, contentRedactionType: ContentRedactionType? = nil, enableChannelIdentification: Bool? = nil, enablePartialResultsStabilization: Bool? = nil, identifyLanguage: Bool? = nil, languageCode: LanguageCode? = nil, languageModelName: String? = nil, languageOptions: String? = nil, mediaEncoding: MediaEncoding, mediaSampleRateHertz: Int, numberOfChannels: Int? = nil, partialResultsStability: PartialResultsStability? = nil, piiEntityTypes: String? = nil, preferredLanguage: LanguageCode? = nil, sessionId: String? = nil, showSpeakerLabel: Bool? = nil, vocabularyFilterMethod: VocabularyFilterMethod? = nil, vocabularyFilterName: String? = nil, vocabularyFilterNames: String? = nil, vocabularyName: String? = nil, vocabularyNames: String? = nil) {
            self.audioStream = audioStream
            self.contentIdentificationType = contentIdentificationType
            self.contentRedactionType = contentRedactionType
            self.enableChannelIdentification = enableChannelIdentification
            self.enablePartialResultsStabilization = enablePartialResultsStabilization
            self.identifyLanguage = identifyLanguage
            self.languageCode = languageCode
            self.languageModelName = languageModelName
            self.languageOptions = languageOptions
            self.mediaEncoding = mediaEncoding
            self.mediaSampleRateHertz = mediaSampleRateHertz
            self.numberOfChannels = numberOfChannels
            self.partialResultsStability = partialResultsStability
            self.piiEntityTypes = piiEntityTypes
            self.preferredLanguage = preferredLanguage
            self.sessionId = sessionId
            self.showSpeakerLabel = showSpeakerLabel
            self.vocabularyFilterMethod = vocabularyFilterMethod
            self.vocabularyFilterName = vocabularyFilterName
            self.vocabularyFilterNames = vocabularyFilterNames
            self.vocabularyName = vocabularyName
            self.vocabularyNames = vocabularyNames
        }

        public func validate(name: String) throws {
            try self.validate(self.languageModelName, name: "languageModelName", parent: name, max: 200)
            try self.validate(self.languageModelName, name: "languageModelName", parent: name, min: 1)
            try self.validate(self.languageModelName, name: "languageModelName", parent: name, pattern: "^[0-9a-zA-Z._-]+$")
            try self.validate(self.languageOptions, name: "languageOptions", parent: name, max: 200)
            try self.validate(self.languageOptions, name: "languageOptions", parent: name, min: 1)
            try self.validate(self.languageOptions, name: "languageOptions", parent: name, pattern: "^[a-zA-Z-,]+$")
            try self.validate(self.mediaSampleRateHertz, name: "mediaSampleRateHertz", parent: name, max: 48000)
            try self.validate(self.mediaSampleRateHertz, name: "mediaSampleRateHertz", parent: name, min: 8000)
            try self.validate(self.numberOfChannels, name: "numberOfChannels", parent: name, min: 2)
            try self.validate(self.piiEntityTypes, name: "piiEntityTypes", parent: name, max: 300)
            try self.validate(self.piiEntityTypes, name: "piiEntityTypes", parent: name, min: 1)
            try self.validate(self.piiEntityTypes, name: "piiEntityTypes", parent: name, pattern: "^[A-Z_, ]+$")
            try self.validate(self.sessionId, name: "sessionId", parent: name, max: 36)
            try self.validate(self.sessionId, name: "sessionId", parent: name, min: 36)
            try self.validate(self.sessionId, name: "sessionId", parent: name, pattern: "^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$")
            try self.validate(self.vocabularyFilterName, name: "vocabularyFilterName", parent: name, max: 200)
            try self.validate(self.vocabularyFilterName, name: "vocabularyFilterName", parent: name, min: 1)
            try self.validate(self.vocabularyFilterName, name: "vocabularyFilterName", parent: name, pattern: "^[0-9a-zA-Z._-]+$")
            try self.validate(self.vocabularyFilterNames, name: "vocabularyFilterNames", parent: name, max: 3000)
            try self.validate(self.vocabularyFilterNames, name: "vocabularyFilterNames", parent: name, min: 1)
            try self.validate(self.vocabularyFilterNames, name: "vocabularyFilterNames", parent: name, pattern: "^[a-zA-Z0-9,-._]+$")
            try self.validate(self.vocabularyName, name: "vocabularyName", parent: name, max: 200)
            try self.validate(self.vocabularyName, name: "vocabularyName", parent: name, min: 1)
            try self.validate(self.vocabularyName, name: "vocabularyName", parent: name, pattern: "^[0-9a-zA-Z._-]+$")
            try self.validate(self.vocabularyNames, name: "vocabularyNames", parent: name, max: 3000)
            try self.validate(self.vocabularyNames, name: "vocabularyNames", parent: name, min: 1)
            try self.validate(self.vocabularyNames, name: "vocabularyNames", parent: name, pattern: "^[a-zA-Z0-9,-._]+$")
        }

        private enum CodingKeys: String, CodingKey {
            case audioStream = "AudioStream"
        }
    }

    public struct StartStreamTranscriptionResponse: AWSDecodableShape & AWSShapeWithPayload {
        /// The key for the payload
        public static let _payloadPath: String = "transcriptResultStream"
        public static var _encoding = [
            AWSMemberEncoding(label: "contentIdentificationType", location: .header("x-amzn-transcribe-content-identification-type")),
            AWSMemberEncoding(label: "contentRedactionType", location: .header("x-amzn-transcribe-content-redaction-type")),
            AWSMemberEncoding(label: "enableChannelIdentification", location: .header("x-amzn-transcribe-enable-channel-identification")),
            AWSMemberEncoding(label: "enablePartialResultsStabilization", location: .header("x-amzn-transcribe-enable-partial-results-stabilization")),
            AWSMemberEncoding(label: "identifyLanguage", location: .header("x-amzn-transcribe-identify-language")),
            AWSMemberEncoding(label: "languageCode", location: .header("x-amzn-transcribe-language-code")),
            AWSMemberEncoding(label: "languageModelName", location: .header("x-amzn-transcribe-language-model-name")),
            AWSMemberEncoding(label: "languageOptions", location: .header("x-amzn-transcribe-language-options")),
            AWSMemberEncoding(label: "mediaEncoding", location: .header("x-amzn-transcribe-media-encoding")),
            AWSMemberEncoding(label: "mediaSampleRateHertz", location: .header("x-amzn-transcribe-sample-rate")),
            AWSMemberEncoding(label: "numberOfChannels", location: .header("x-amzn-transcribe-number-of-channels")),
            AWSMemberEncoding(label: "partialResultsStability", location: .header("x-amzn-transcribe-partial-results-stability")),
            AWSMemberEncoding(label: "piiEntityTypes", location: .header("x-amzn-transcribe-pii-entity-types")),
            AWSMemberEncoding(label: "preferredLanguage", location: .header("x-amzn-transcribe-preferred-language")),
            AWSMemberEncoding(label: "requestId", location: .header("x-amzn-request-id")),
            AWSMemberEncoding(label: "sessionId", location: .header("x-amzn-transcribe-session-id")),
            AWSMemberEncoding(label: "showSpeakerLabel", location: .header("x-amzn-transcribe-show-speaker-label")),
            AWSMemberEncoding(label: "transcriptResultStream", location: .body("TranscriptResultStream")),
            AWSMemberEncoding(label: "vocabularyFilterMethod", location: .header("x-amzn-transcribe-vocabulary-filter-method")),
            AWSMemberEncoding(label: "vocabularyFilterName", location: .header("x-amzn-transcribe-vocabulary-filter-name")),
            AWSMemberEncoding(label: "vocabularyFilterNames", location: .header("x-amzn-transcribe-vocabulary-filter-names")),
            AWSMemberEncoding(label: "vocabularyName", location: .header("x-amzn-transcribe-vocabulary-name")),
            AWSMemberEncoding(label: "vocabularyNames", location: .header("x-amzn-transcribe-vocabulary-names"))
        ]

        /// Shows whether content identification was enabled in this stream.
        public let contentIdentificationType: ContentIdentificationType?
        /// Shows whether content redaction was enabled in this stream.
        public let contentRedactionType: ContentRedactionType?
        /// Shows whether channel identification was enabled in the stream.
        public let enableChannelIdentification: Bool?
        /// Shows whether partial results stabilization was enabled in the transcription.
        public let enablePartialResultsStabilization: Bool?
        /// The language code of the language identified in your media stream.
        public let identifyLanguage: Bool?
        /// The language code of the input audio stream.
        public let languageCode: LanguageCode?
        /// The name of the custom language model used in the transcription.
        public let languageModelName: String?
        /// The language codes used in the identification of your media stream's predominant  language.
        public let languageOptions: String?
        /// The encoding used for the input audio stream.
        public let mediaEncoding: MediaEncoding?
        /// The sample rate, in Hertz (Hz), for the input audio stream.
        public let mediaSampleRateHertz: Int?
        /// The number of channels identified in the stream.
        public let numberOfChannels: Int?
        /// If partial results stabilization has been enabled in the stream, shows the stability level.
        public let partialResultsStability: PartialResultsStability?
        /// Lists the PII entity types you specified in your request.
        public let piiEntityTypes: String?
        /// The preferred language you specified in your request.
        public let preferredLanguage: LanguageCode?
        /// An identifier for the transcription.
        public let requestId: String?
        /// An identifier for a specific transcription session.
        public let sessionId: String?
        /// Shows whether speaker identification was enabled in the transcription.
        public let showSpeakerLabel: Bool?
        /// Represents the stream of transcription events from Amazon Transcribe to your application.
        public let transcriptResultStream: TranscriptResultStream?
        /// The vocabulary filtering method used when processing the stream.
        public let vocabularyFilterMethod: VocabularyFilterMethod?
        /// The name of the vocabulary filter used when processing the stream.
        public let vocabularyFilterName: String?
        /// The name of the vocabulary filter used when processing the stream.
        public let vocabularyFilterNames: String?
        /// The name of the custom vocabulary used when processing the stream.
        public let vocabularyName: String?
        /// The name of the custom vocabulary used when processing the stream.
        public let vocabularyNames: String?

        public init(contentIdentificationType: ContentIdentificationType? = nil, contentRedactionType: ContentRedactionType? = nil, enableChannelIdentification: Bool? = nil, enablePartialResultsStabilization: Bool? = nil, identifyLanguage: Bool? = nil, languageCode: LanguageCode? = nil, languageModelName: String? = nil, languageOptions: String? = nil, mediaEncoding: MediaEncoding? = nil, mediaSampleRateHertz: Int? = nil, numberOfChannels: Int? = nil, partialResultsStability: PartialResultsStability? = nil, piiEntityTypes: String? = nil, preferredLanguage: LanguageCode? = nil, requestId: String? = nil, sessionId: String? = nil, showSpeakerLabel: Bool? = nil, transcriptResultStream: TranscriptResultStream? = nil, vocabularyFilterMethod: VocabularyFilterMethod? = nil, vocabularyFilterName: String? = nil, vocabularyFilterNames: String? = nil, vocabularyName: String? = nil, vocabularyNames: String? = nil) {
            self.contentIdentificationType = contentIdentificationType
            self.contentRedactionType = contentRedactionType
            self.enableChannelIdentification = enableChannelIdentification
            self.enablePartialResultsStabilization = enablePartialResultsStabilization
            self.identifyLanguage = identifyLanguage
            self.languageCode = languageCode
            self.languageModelName = languageModelName
            self.languageOptions = languageOptions
            self.mediaEncoding = mediaEncoding
            self.mediaSampleRateHertz = mediaSampleRateHertz
            self.numberOfChannels = numberOfChannels
            self.partialResultsStability = partialResultsStability
            self.piiEntityTypes = piiEntityTypes
            self.preferredLanguage = preferredLanguage
            self.requestId = requestId
            self.sessionId = sessionId
            self.showSpeakerLabel = showSpeakerLabel
            self.transcriptResultStream = transcriptResultStream
            self.vocabularyFilterMethod = vocabularyFilterMethod
            self.vocabularyFilterName = vocabularyFilterName
            self.vocabularyFilterNames = vocabularyFilterNames
            self.vocabularyName = vocabularyName
            self.vocabularyNames = vocabularyNames
        }

        private enum CodingKeys: String, CodingKey {
            case contentIdentificationType = "x-amzn-transcribe-content-identification-type"
            case contentRedactionType = "x-amzn-transcribe-content-redaction-type"
            case enableChannelIdentification = "x-amzn-transcribe-enable-channel-identification"
            case enablePartialResultsStabilization = "x-amzn-transcribe-enable-partial-results-stabilization"
            case identifyLanguage = "x-amzn-transcribe-identify-language"
            case languageCode = "x-amzn-transcribe-language-code"
            case languageModelName = "x-amzn-transcribe-language-model-name"
            case languageOptions = "x-amzn-transcribe-language-options"
            case mediaEncoding = "x-amzn-transcribe-media-encoding"
            case mediaSampleRateHertz = "x-amzn-transcribe-sample-rate"
            case numberOfChannels = "x-amzn-transcribe-number-of-channels"
            case partialResultsStability = "x-amzn-transcribe-partial-results-stability"
            case piiEntityTypes = "x-amzn-transcribe-pii-entity-types"
            case preferredLanguage = "x-amzn-transcribe-preferred-language"
            case requestId = "x-amzn-request-id"
            case sessionId = "x-amzn-transcribe-session-id"
            case showSpeakerLabel = "x-amzn-transcribe-show-speaker-label"
            case transcriptResultStream = "TranscriptResultStream"
            case vocabularyFilterMethod = "x-amzn-transcribe-vocabulary-filter-method"
            case vocabularyFilterName = "x-amzn-transcribe-vocabulary-filter-name"
            case vocabularyFilterNames = "x-amzn-transcribe-vocabulary-filter-names"
            case vocabularyName = "x-amzn-transcribe-vocabulary-name"
            case vocabularyNames = "x-amzn-transcribe-vocabulary-names"
        }
    }

    public struct Transcript: AWSDecodableShape {
        ///  Result objects that contain the results of transcribing a portion of the input audio stream. The array can be empty.
        public let results: [Result]?

        public init(results: [Result]? = nil) {
            self.results = results
        }

        private enum CodingKeys: String, CodingKey {
            case results = "Results"
        }
    }

    public struct TranscriptEvent: AWSDecodableShape {
        /// The transcription of the audio stream. The transcription is composed of all of the items in the results list.
        public let transcript: Transcript?

        public init(transcript: Transcript? = nil) {
            self.transcript = transcript
        }

        private enum CodingKeys: String, CodingKey {
            case transcript = "Transcript"
        }
    }

    public struct AudioStream: AWSEncodableShape {
        /// A blob of audio from your application. You audio stream consists of one or more audio events. For information on audio encoding formats in Amazon Transcribe, see Speech input. For information on audio encoding formats in Amazon Transcribe Medical, see Speech input. For more information on stream encoding in Amazon Transcribe, see Event stream encoding. For information on stream encoding in Amazon Transcribe Medical, see Event stream encoding.
        public let audioEvent: AudioEvent?

        public init(audioEvent: AudioEvent? = nil) {
            self.audioEvent = audioEvent
        }

        private enum CodingKeys: String, CodingKey {
            case audioEvent = "AudioEvent"
        }
    }
}
